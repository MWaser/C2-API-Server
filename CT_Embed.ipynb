{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMHxnpb+y7z/YeMMcbqsb3p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MWaser/C2-API-Server/blob/master/CT_Embed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vector Embedding & Search/Retrieval\n",
        "\n",
        "First half of notebook is for creating files to load embedding data into Snowflake\n",
        "\n",
        "Second half is for testing semantic retrieval after the embedding data is loaded\n",
        "\n",
        "Both could be done from within SnowFlake if we have a non-Government region\n",
        "but <br />creating large files in Snowflake is EXPENSIVE since each encode call takes 1/10 of a second."
      ],
      "metadata": {
        "id": "LtzvPdt2U0hS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialization\n",
        "\n",
        "The following needs to be run at the start of every session whether creating embedding files or testing semantic retrieval"
      ],
      "metadata": {
        "id": "0iPXrUDqsLTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install snowflake-connector-python\n",
        "import snowflake.connector\n",
        "!pip install -U sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "import numpy\n",
        "import pandas as pd\n",
        "import time"
      ],
      "metadata": {
        "id": "wONzbqJipcKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive')"
      ],
      "metadata": {
        "id": "R8gGa_lHwDtJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a57465e-fd07-4df7-f55a-a3f123e1eca3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Embedding Data File(s) for loading into Snowflake\n",
        "\n",
        "There are currently 144K clinical trials that are either recruiting or awaiting recruiting.  \n",
        "These have been placed in the C2_CTRIAL.CLINICAL_TRIALS.CT_SEARCH data set.\n",
        "\n",
        "Snowflake has a size limit of 16MB when loading JSON files which only allows about 3K records/batch."
      ],
      "metadata": {
        "id": "ohS1iU7NVInO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rowCount = 145000\n",
        "batchSize = 3000\n",
        "filename = 'embed'"
      ],
      "metadata": {
        "id": "5vz6ZX_D8xAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# modify the query and the marked variable assignment block as appropriate\n",
        "startRow = 1\n",
        "endRow = batchSize\n",
        "while startRow <= rowCount:\n",
        "    fname = '/drive/My Drive/' + filename + str(startRow) + '.json'\n",
        "    print(time.strftime(\"%H:%M:%S\", time.localtime()) + '  ' + fname)\n",
        "    cnx = snowflake.connector.connect(user='waserma', password='snowH34d',\n",
        "            account='csi_eval.us-east-1-gov.aws',\n",
        "            warehouse='WAREHOUSE1' )\n",
        "    curs = cnx.cursor()\n",
        "    query = 'SELECT * FROM C2_CTRIAL.CLINICAL_TRIALS.CT_SEARCH WHERE ROWNUM >= ' + str(startRow) + ' AND ROWNUM <= ' + str(endRow) + ' ORDER BY ROWNUM'\n",
        "    snowData = curs.execute(query).fetchall()\n",
        "    column_names = [i[0] for i in curs.description]\n",
        "    curs.close()\n",
        "    tempDF = pd.DataFrame(snowData, columns=column_names)\n",
        "    # BEGIN MODIFY\n",
        "    tempDF['BRIEF_TITLE'] = tempDF['BRIEF_TITLE'].astype('str')\n",
        "    tempDF['OFFICIAL_TITLE'] = tempDF['OFFICIAL_TITLE'].astype('str')\n",
        "    tempDF['DESCRIPTION'] = tempDF['DESCRIPTION'].astype('str')\n",
        "    tempDF['DOWNCASE_NAMES'] = tempDF['DOWNCASE_NAMES'].astype('str')\n",
        "    tempDF['DOWNCASE_MESH_TERMS'] = tempDF['DOWNCASE_MESH_TERMS'].astype('str')\n",
        "    tempDF['EMBED'] = tempDF.apply(lambda row: model.encode(row['BRIEF_TITLE'] + row['OFFICIAL_TITLE'] + row['DESCRIPTION'] + row['DOWNCASE_NAMES'] + row['DOWNCASE_MESH_TERMS']), axis=1)\n",
        "    tempDF.drop(['ROWNUM', 'BRIEF_TITLE', 'OFFICIAL_TITLE', 'DESCRIPTION', 'DOWNCASE_NAMES', 'DOWNCASE_MESH_TERMS'], axis=1, inplace=True)\n",
        "    # END MODIFY\n",
        "    tempDF.to_json(fname, orient=\"records\")\n",
        "    startRow = startRow + batchSize\n",
        "    endRow = endRow + batchSize\n",
        "print(\"done!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJ3Pea-dsnLQ",
        "outputId": "26771ca1-d79e-4e8a-8864-0b576e0033dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19:09:45  /drive/My Drive/embed1.json\n",
            "done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Semantic Search/Retrieval\n",
        "\n",
        "Waiting to complete Sonflake side . . ."
      ],
      "metadata": {
        "id": "J_PFkfIWqeX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "searchStr = input('What is your medical malfunction?  ')\n",
        "searchVec = model.encode(searchStr)\n",
        "finalStr = numpy.array_repr(searchVec).replace(\"\\n\",\"\").replace(\"array(\", \"\").replace(\",      dtype=float32)\",\"\").replace(\"       \", \" \").replace(\"  \", \" \")\n",
        "print(finalStr)\n",
        "# print(time.strftime(\"%H:%M:%S\", time.localtime()) + '  start search')\n",
        "# print(searchVec)\n",
        "cnx = snowflake.connector.connect(user='waserma', password='snowH34d',\n",
        "        account='csi_eval.us-east-1-gov.aws',\n",
        "        warehouse='WAREHOUSE1' )\n",
        "curs = cnx.cursor()\n",
        "query = 'SELECT * FROM C2_CTRIAL.CLINICAL_TRIALS.CT_SEARCH WHERE ROWNUM >= 1 AND ROWNUM <= 10 ORDER BY ROWNUM'\n",
        "snowData = curs.execute(query).fetchall()\n",
        "column_names = [i[0] for i in curs.description]\n",
        "curs.close()\n",
        "tempDF = pd.DataFrame(snowData, columns=column_names)\n",
        "tempDF.head()\n",
        "# print(time.strftime(\"%H:%M:%S\", time.localtime()) + '  done')\n"
      ],
      "metadata": {
        "id": "EI04YvZNqjpB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}